{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1df3cd-9761-430d-a045-392c1a9e5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c739fda-04a5-4359-985b-eed37286c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory: C:\\Users\\Atharva Kulkarni\\Desktop\\Programming\\ML PROJECTS\\SIEM\\SIEM project\\AI-driven-SIEM-System\n"
     ]
    }
   ],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "os.chdir(project_root)  # 👈 this changes the working directory\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"New working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ea5a7f-1fd2-493f-a2df-08d5aa5d9ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: data/logs/raw/\n"
     ]
    }
   ],
   "source": [
    "from src.utils.config_loader import load_config\n",
    "config = load_config(\"configs/log-anomaly-detection.yml\")\n",
    "print(\"Raw data path:\", config[\"data_paths\"][\"input_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34e234-8f96-4d74-9a7a-7d4bfd8050bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9dc6c0-dd56-441f-9b62-6929c4d8aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== DATA PROCESSING ==================\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=8, stride=8, return_indices=False):\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.data = data\n",
    "        self.return_indices = return_indices\n",
    "        self.samples = []\n",
    "        self.indices = []\n",
    "        \n",
    "        for i in range(0, len(data)-seq_len, stride):\n",
    "            self.samples.append(data[i:i+seq_len])\n",
    "            self.indices.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx], dtype=torch.float)\n",
    "        if self.return_indices:\n",
    "            return sample, self.indices[idx]\n",
    "        return sample\n",
    "\n",
    "def load_and_preprocess(path):\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Store original data for reference\n",
    "    original_df = df.copy()\n",
    "    \n",
    "    # Remove only clearly problematic columns\n",
    "    columns_to_drop = [\"LineId\", \"Time\"]  # Keep Content and EventTemplate for now\n",
    "    \n",
    "    # Remove temporal identifiers that cause overfitting\n",
    "    if \"Date\" in df.columns:\n",
    "        columns_to_drop.append(\"Date\")\n",
    "        print(\"Removed 'Date' column - temporal identifiers cause overfitting\")\n",
    "    \n",
    "    if \"PID\" in df.columns:\n",
    "        columns_to_drop.append(\"PID\")\n",
    "        print(\"Removed 'PID' column - process IDs are not semantically meaningful\")\n",
    "    \n",
    "    print(f\"Dropping columns: {columns_to_drop}\")\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    # Define categorical columns properly\n",
    "    categorical_cols = [\"Level\", \"Component\", \"EventId\"]\n",
    "    \n",
    "    # Add EventTemplate if it exists and has reasonable cardinality\n",
    "    if \"EventTemplate\" in df.columns:\n",
    "        unique_templates = df[\"EventTemplate\"].nunique()\n",
    "        print(f\"EventTemplate has {unique_templates} unique values\")\n",
    "        \n",
    "        if unique_templates < 1000:  # Reasonable threshold\n",
    "            categorical_cols.append(\"EventTemplate\")\n",
    "            print(\"Including EventTemplate as categorical feature\")\n",
    "        else:\n",
    "            print(f\"EventTemplate has too many unique values ({unique_templates}), dropping\")\n",
    "            df = df.drop(columns=[\"EventTemplate\"])\n",
    "    \n",
    "    # Handle Content with better feature extraction\n",
    "    if \"Content\" in df.columns:\n",
    "        print(\"Processing Content column...\")\n",
    "        \n",
    "        # Basic length and word count features\n",
    "        df['content_length'] = df['Content'].str.len()\n",
    "        df['content_word_count'] = df['Content'].str.split().str.len()\n",
    "        \n",
    "        # More discriminative error/warning detection with word boundaries\n",
    "        df['content_has_error'] = df['Content'].str.contains(\n",
    "            r'\\b(error|fail|exception|crash|abort|fault)\\b', case=False, na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        df['content_has_warning'] = df['Content'].str.contains(\n",
    "            r'\\b(warn|alert|caution)\\b', case=False, na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        # More specific numeric patterns\n",
    "        df['content_has_large_numbers'] = df['Content'].str.contains(\n",
    "            r'\\b\\d{4,}\\b', na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Critical system indicators\n",
    "        df['content_has_critical'] = df['Content'].str.contains(\n",
    "            r'\\b(critical|fatal|panic|segfault|timeout|killed)\\b', case=False, na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Network/security related patterns\n",
    "        df['content_has_network'] = df['Content'].str.contains(\n",
    "            r'\\b(connection|socket|port|network|tcp|udp)\\b', case=False, na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Memory/resource related patterns\n",
    "        df['content_has_memory'] = df['Content'].str.contains(\n",
    "            r'\\b(memory|malloc|free|leak|oom|out of memory)\\b', case=False, na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Drop the original Content column\n",
    "        df = df.drop(columns=[\"Content\"])\n",
    "        print(\"Extracted features: length, word_count, has_error, has_warning, has_large_numbers, has_critical, has_network, has_memory\")\n",
    "        \n",
    "        # Validation: Check feature distributions\n",
    "        content_features = ['content_has_error', 'content_has_warning', 'content_has_large_numbers', \n",
    "                           'content_has_critical', 'content_has_network', 'content_has_memory']\n",
    "        for feature in content_features:\n",
    "            if feature in df.columns:\n",
    "                percentage = df[feature].mean() * 100\n",
    "                print(f\"  {feature}: {percentage:.1f}% of logs\")\n",
    "    \n",
    "    # Remove Month entirely\n",
    "    if \"Month\" in df.columns:\n",
    "        df = df.drop(columns=[\"Month\"])\n",
    "        print(\"Removed 'Month' column - avoiding temporal overfitting\")\n",
    "    \n",
    "    # Filter to only existing categorical columns\n",
    "    categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "    print(f\"Using categorical columns: {categorical_cols}\")\n",
    "    \n",
    "    if categorical_cols:\n",
    "        if \"EventTemplate\" in categorical_cols:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            \n",
    "            print(\"Processing EventTemplate with TF-IDF for better semantic representation...\")\n",
    "            \n",
    "            # Custom stopwords for log data\n",
    "            log_stopwords = [\n",
    "                'at', 'by', 'for', 'from', 'to', 'in', 'on', 'with', 'of', 'the', 'a', 'an',\n",
    "                'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "                'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',\n",
    "                'and', 'or', 'but', 'if', 'then', 'else', 'when', 'where', 'why', 'how',\n",
    "                'this', 'that', 'these', 'those', 'it', 'its', 'they', 'them', 'their'\n",
    "            ]\n",
    "            \n",
    "            # Use TF-IDF for EventTemplate\n",
    "            tfidf = TfidfVectorizer(\n",
    "                max_features=50,\n",
    "                stop_words=log_stopwords,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                max_df=0.7,\n",
    "                token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n",
    "            )\n",
    "            \n",
    "            template_tfidf = tfidf.fit_transform(df[\"EventTemplate\"].astype(str))\n",
    "            template_features = template_tfidf.toarray()\n",
    "            \n",
    "            print(f\"EventTemplate TF-IDF generated {template_features.shape[1]} meaningful features\")\n",
    "            \n",
    "            # Process other categoricals normally\n",
    "            other_cats = [col for col in categorical_cols if col != \"EventTemplate\"]\n",
    "            if other_cats:\n",
    "                other_ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=50)\n",
    "                other_features = other_ohe.fit_transform(df[other_cats].astype(str))\n",
    "                cat_features = np.hstack([template_features, other_features])\n",
    "                print(f\"Other categoricals generated {other_features.shape[1]} features\")\n",
    "                \n",
    "                ohe = (tfidf, other_ohe)\n",
    "            else:\n",
    "                cat_features = template_features\n",
    "                ohe = (tfidf, None)\n",
    "            \n",
    "            df = df.drop(columns=[\"EventTemplate\"])\n",
    "        else:\n",
    "            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=100)\n",
    "            cat_features = ohe.fit_transform(df[categorical_cols].astype(str))\n",
    "        \n",
    "        df = df.drop(columns=[col for col in categorical_cols if col != \"EventTemplate\"])\n",
    "    else:\n",
    "        ohe = None\n",
    "        cat_features = np.array([]).reshape(len(df), 0)\n",
    "\n",
    "    # Process numerical features\n",
    "    numerical_cols = list(df.columns)\n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "    \n",
    "    if len(numerical_cols) > 0:\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        num_features = imputer.fit_transform(df[numerical_cols])\n",
    "    else:\n",
    "        imputer = None\n",
    "        num_features = np.array([]).reshape(len(df), 0)\n",
    "\n",
    "    # Feature combination\n",
    "    if cat_features.shape[1] > 0 and num_features.shape[1] > 0:\n",
    "        combined = np.hstack([cat_features, num_features])\n",
    "    elif cat_features.shape[1] > 0:\n",
    "        combined = cat_features\n",
    "    elif num_features.shape[1] > 0:\n",
    "        combined = num_features\n",
    "    else:\n",
    "        raise ValueError(\"No features remaining after preprocessing!\")\n",
    "    \n",
    "    # Apply scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(combined)\n",
    "    \n",
    "    print(f\"Final feature dimension: {scaled_data.shape[1]}\")\n",
    "    print(f\"Numerical features: {num_features.shape[1]}, Categorical features: {cat_features.shape[1]}\")\n",
    "\n",
    "    return scaled_data, ohe, imputer, scaler, original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f43253-e82b-4811-a629-7abc16d3607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== HYBRID ATTENTION LSTM AUTOENCODER ==================\n",
    "class HybridAttentionLSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=16, dropout=0.4, enable_single_log=True):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.enable_single_log = enable_single_log\n",
    "        \n",
    "        # Sequential processing components (original approach)\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True, \n",
    "                              num_layers=2, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, \n",
    "                                             dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.decoder = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, \n",
    "                              num_layers=2, dropout=dropout)\n",
    "        \n",
    "        # Single log processing components (new approach)\n",
    "        if enable_single_log:\n",
    "            self.single_log_encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "            )\n",
    "            \n",
    "            self.single_log_decoder = nn.Sequential(\n",
    "                nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim * 2, input_dim)\n",
    "            )\n",
    "        \n",
    "        # Shared output layers\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fusion layer for combining both approaches\n",
    "        self.fusion_layer = nn.Linear(input_dim * 2, input_dim)\n",
    "        \n",
    "    def forward(self, x, mode='hybrid'):\n",
    "        \"\"\"\n",
    "        Forward pass with multiple modes:\n",
    "        - 'sequential': Original sequence-based processing only\n",
    "        - 'single': Single log processing only  \n",
    "        - 'hybrid': Both approaches combined (default)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        \n",
    "        if mode == 'single' or (mode == 'hybrid' and self.enable_single_log):\n",
    "            # Single log processing path\n",
    "            single_outputs = []\n",
    "            single_attentions = []\n",
    "            \n",
    "            for i in range(seq_len):\n",
    "                log_features = x[:, i, :]  # Shape: (batch_size, input_dim)\n",
    "                \n",
    "                # Process single log\n",
    "                encoded_single = self.single_log_encoder(log_features)\n",
    "                decoded_single = self.single_log_decoder(encoded_single)\n",
    "                \n",
    "                single_outputs.append(decoded_single)\n",
    "                # Create dummy attention for single logs (uniform attention)\n",
    "                dummy_attention = torch.ones(batch_size, 1, 1) / seq_len\n",
    "                single_attentions.append(dummy_attention)\n",
    "            \n",
    "            single_reconstruction = torch.stack(single_outputs, dim=1)\n",
    "            single_attention_weights = torch.cat(single_attentions, dim=2)\n",
    "        \n",
    "        if mode == 'sequential' or mode == 'hybrid':\n",
    "            # Sequential processing path (original)\n",
    "            enc_out, (hidden, cell) = self.encoder(x)\n",
    "            attn_out, attn_weights = self.attention(enc_out, enc_out, enc_out)\n",
    "            attn_out = self.dropout(attn_out)\n",
    "            dec_out, _ = self.decoder(attn_out)\n",
    "            \n",
    "            # Batch normalization and output\n",
    "            dec_out_norm = self.batch_norm(dec_out.transpose(1, 2)).transpose(1, 2)\n",
    "            sequential_reconstruction = self.output_layer(dec_out_norm)\n",
    "        \n",
    "        # Return based on mode\n",
    "        if mode == 'single':\n",
    "            return single_reconstruction, single_attention_weights\n",
    "        elif mode == 'sequential':\n",
    "            return sequential_reconstruction, attn_weights\n",
    "        else:  # hybrid mode\n",
    "            # Combine both reconstructions\n",
    "            combined_features = torch.cat([sequential_reconstruction, single_reconstruction], dim=-1)\n",
    "            hybrid_reconstruction = self.fusion_layer(combined_features)\n",
    "            \n",
    "            # Combine attention weights (weighted average)\n",
    "            combined_attention = 0.6 * attn_weights + 0.4 * single_attention_weights.expand_as(attn_weights)\n",
    "            \n",
    "            return hybrid_reconstruction, combined_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3139e-df66-4695-a7fc-502ee277790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== SEVERITY CLASSIFICATION ==================\n",
    "class EnhancedSeverityManager:\n",
    "    def __init__(self, percentiles=None, severity_labels=None):\n",
    "        self.percentiles = percentiles or [85, 95, 99]\n",
    "        self.severity_labels = severity_labels or ['Low', 'Medium', 'High', 'Critical']\n",
    "        self.threshold_values = {}\n",
    "        self.error_stats = {}\n",
    "        \n",
    "    def learn_thresholds(self, error_distribution, validation_errors=None):\n",
    "        \"\"\"Learn thresholds with optional validation for stability\"\"\"\n",
    "        error_array = np.array(error_distribution)\n",
    "        \n",
    "        # Learn primary thresholds\n",
    "        for p in self.percentiles:\n",
    "            self.threshold_values[f'p{p}'] = np.percentile(error_array, p)\n",
    "        \n",
    "        # Store distribution statistics\n",
    "        self.error_stats = {\n",
    "            'mean': np.mean(error_array),\n",
    "            'std': np.std(error_array),\n",
    "            'median': np.median(error_array),\n",
    "            'iqr': np.percentile(error_array, 75) - np.percentile(error_array, 25)\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Learned severity thresholds: {self.threshold_values}\")\n",
    "        print(f\"📊 Error distribution stats: {self.error_stats}\")\n",
    "    \n",
    "    def classify_with_confidence(self, error):\n",
    "        \"\"\"Classify severity with confidence score\"\"\"\n",
    "        if not self.threshold_values:\n",
    "            raise RuntimeError(\"Thresholds not learned. Call learn_thresholds() first.\")\n",
    "        \n",
    "        # Determine severity level\n",
    "        severity_idx = 0\n",
    "        for i, p in enumerate(self.percentiles):\n",
    "            if error > self.threshold_values[f'p{p}']:\n",
    "                severity_idx = i + 1\n",
    "        \n",
    "        severity = self.severity_labels[severity_idx]\n",
    "        \n",
    "        # Calculate confidence based on distance from threshold\n",
    "        if severity_idx == 0:\n",
    "            threshold = self.threshold_values[f'p{self.percentiles[0]}']\n",
    "            confidence = max(0.1, 1.0 - (error / threshold))\n",
    "        else:\n",
    "            current_threshold = self.threshold_values[f'p{self.percentiles[severity_idx-1]}']\n",
    "            confidence = min(1.0, (error - current_threshold) / current_threshold + 0.5)\n",
    "        \n",
    "        return severity, min(1.0, max(0.1, confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1ef22-babe-4f6e-a01e-84b6a7bd05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== RULE-BASED LOG TYPE CLASSIFIER ==================\n",
    "class RuleBasedLogClassifier:\n",
    "    def __init__(self):\n",
    "        self.classification_rules = {\n",
    "            'memory_error': [\n",
    "                r'\\b(out of memory|oom|page allocation failure|dma timeout)\\b',\n",
    "                r'\\b(malloc failed|memory leak|segfault|kernel panic)\\b',\n",
    "                r'\\b(swap.*full|virtual memory|memory pressure)\\b'\n",
    "            ],\n",
    "            'authentication_error': [\n",
    "                r'\\b(authentication failure|invalid username|login failed)\\b',\n",
    "                r'\\b(kerberos.*failed|pam_unix.*failed|ssh.*failed)\\b',\n",
    "                r'\\b(password.*incorrect|access denied|unauthorized)\\b'\n",
    "            ],\n",
    "            'filesystem_error': [\n",
    "                r'\\b(no such file|permission denied|disk full|quota exceeded)\\b',\n",
    "                r'\\b(failed command|status timeout|drive not ready|io error)\\b',\n",
    "                r'\\b(filesystem.*corrupt|bad sector|read.*error)\\b'\n",
    "            ],\n",
    "            'network_error': [\n",
    "                r'\\b(connection timed out|connection refused|peer died)\\b',\n",
    "                r'\\b(network unreachable|socket error|host.*down)\\b',\n",
    "                r'\\b(dns.*failed|routing.*error|packet.*lost)\\b'\n",
    "            ],\n",
    "            'permission_error': [\n",
    "                r'\\b(permission denied|operation not supported|access forbidden)\\b',\n",
    "                r'\\b(selinux.*denied|capability.*denied|privilege.*error)\\b',\n",
    "                r'\\b(sudo.*failed|su.*failed|root.*access)\\b'\n",
    "            ],\n",
    "            'system_critical': [\n",
    "                r'\\b(critical|fatal|panic|emergency|alert)\\b',\n",
    "                r'\\b(system.*halt|kernel.*oops|hardware.*error)\\b',\n",
    "                r'\\b(temperature.*critical|power.*failure)\\b'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.pattern_weights = {\n",
    "            'memory_error': 0.9,\n",
    "            'authentication_error': 0.95,\n",
    "            'filesystem_error': 0.85,\n",
    "            'network_error': 0.8,\n",
    "            'permission_error': 0.9,\n",
    "            'system_critical': 0.95\n",
    "        }\n",
    "    \n",
    "    def classify_log(self, event_template, content=\"\"):\n",
    "        \"\"\"Classify a single log entry\"\"\"\n",
    "        combined_text = f\"{event_template} {content}\".lower()\n",
    "        \n",
    "        for category, patterns in self.classification_rules.items():\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, combined_text, re.IGNORECASE):\n",
    "                    confidence = self._calculate_confidence(pattern, combined_text, category)\n",
    "                    return {\n",
    "                        'log_type': category,\n",
    "                        'confidence': confidence,\n",
    "                        'matched_pattern': pattern,\n",
    "                        'is_critical': category in ['system_critical', 'authentication_error']\n",
    "                    }\n",
    "        \n",
    "        return {\n",
    "            'log_type': 'normal',\n",
    "            'confidence': 0.7,\n",
    "            'matched_pattern': None,\n",
    "            'is_critical': False\n",
    "        }\n",
    "    \n",
    "    def _calculate_confidence(self, pattern, text, category):\n",
    "        \"\"\"Calculate confidence based on pattern specificity and context\"\"\"\n",
    "        base_confidence = self.pattern_weights.get(category, 0.7)\n",
    "        pattern_specificity = min(len(pattern) / 50.0, 0.3)\n",
    "        keywords = re.findall(r'\\w+', pattern.lower())\n",
    "        keyword_matches = sum(1 for keyword in keywords if keyword in text)\n",
    "        keyword_bonus = min(keyword_matches * 0.05, 0.2)\n",
    "        final_confidence = min(base_confidence + pattern_specificity + keyword_bonus, 0.98)\n",
    "        return round(final_confidence, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fd286-d917-491f-9d46-5b5fb47911b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== HYBRID ENSEMBLE DETECTOR ==================\n",
    "class HybridEnsembleDetector:\n",
    "    def __init__(self, input_dim, num_models=3, enable_single_log=True):\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "        self.input_dim = input_dim\n",
    "        self.enable_single_log = enable_single_log\n",
    "        \n",
    "        # Create diverse hybrid models\n",
    "        configs = [\n",
    "            {'hidden_dim': 16, 'dropout': 0.3},\n",
    "            {'hidden_dim': 24, 'dropout': 0.4},\n",
    "            {'hidden_dim': 32, 'dropout': 0.2}\n",
    "        ]\n",
    "        \n",
    "        for i, config in enumerate(configs[:num_models]):\n",
    "            model = HybridAttentionLSTMAutoencoder(\n",
    "                input_dim, \n",
    "                enable_single_log=enable_single_log,\n",
    "                **config\n",
    "            )\n",
    "            self.models.append(model)\n",
    "            self.weights.append(1.0)\n",
    "    \n",
    "    def train_ensemble(self, train_loader, val_loader, patience=5, max_epochs=100):\n",
    "        model_performances = []\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            print(f\"\\n=== Training Hybrid Model {i+1}/{len(self.models)} ===\")\n",
    "            train_losses, val_losses = self.train_hybrid_model(\n",
    "                model, train_loader, val_loader, patience, max_epochs, \n",
    "                model_name=f'hybrid_ensemble_model_{i}.pth'\n",
    "            )\n",
    "            \n",
    "            final_val_loss = val_losses[-1] if val_losses else float('inf')\n",
    "            model_performances.append(final_val_loss)\n",
    "        \n",
    "        # Update weights based on performance\n",
    "        total_inv_loss = sum(1/loss for loss in model_performances)\n",
    "        self.weights = [(1/loss) / total_inv_loss for loss in model_performances]\n",
    "        \n",
    "        print(f\"\\nHybrid Ensemble weights: {[f'{w:.3f}' for w in self.weights]}\")\n",
    "        return model_performances\n",
    "    \n",
    "    def train_hybrid_model(self, model, train_loader, val_loader, patience=5, max_epochs=100, model_name='hybrid_model.pth'):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        no_improve = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Train with hybrid mode (combines both approaches)\n",
    "                reconstructions, _ = model(batch, mode='hybrid')\n",
    "                loss = criterion(reconstructions, batch)\n",
    "                \n",
    "                # Add regularization for single log consistency\n",
    "                if model.enable_single_log:\n",
    "                    single_recon, _ = model(batch, mode='single')\n",
    "                    seq_recon, _ = model(batch, mode='sequential')\n",
    "                    consistency_loss = 0.1 * criterion(single_recon, seq_recon)\n",
    "                    loss += consistency_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    reconstructions, _ = model(batch, mode='hybrid')\n",
    "                    val_loss += criterion(reconstructions, batch).item()\n",
    "\n",
    "            avg_train = train_loss/len(train_loader)\n",
    "            avg_val = val_loss/len(val_loader)\n",
    "            \n",
    "            train_losses.append(avg_train)\n",
    "            val_losses.append(avg_val)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}: Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}\")\n",
    "            \n",
    "            scheduler.step(avg_val)\n",
    "\n",
    "            if avg_val < best_loss:\n",
    "                best_loss = avg_val\n",
    "                no_improve = 0\n",
    "                torch.save(model.state_dict(), model_name)\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def predict(self, dataloader, mode='hybrid'):\n",
    "        \"\"\"Predict with specified mode\"\"\"\n",
    "        all_errors = []\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            model.load_state_dict(torch.load(f'hybrid_ensemble_model_{i}.pth'))\n",
    "            errors = self.evaluate_hybrid_model(model, dataloader, mode)\n",
    "            all_errors.append(errors)\n",
    "        \n",
    "        # Weighted ensemble prediction\n",
    "        ensemble_errors = np.average(all_errors, axis=0, weights=self.weights)\n",
    "        \n",
    "        return ensemble_errors, all_errors\n",
    "    \n",
    "    def evaluate_hybrid_model(self, model, dataloader, mode='hybrid'):\n",
    "        model.eval()\n",
    "        errors = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                reconstructions, _ = model(batch, mode=mode)\n",
    "                batch_errors = torch.mean((batch - reconstructions)**2, dim=(1,2))\n",
    "                errors.extend(batch_errors.numpy())\n",
    "        return np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d3e0d-db49-491f-a6d8-e6ab45d01b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== OUTPUT PROCESSING FUNCTIONS ==================\n",
    "\n",
    "def process_single_log_outputs(single_errors, single_threshold, test_data, original_df, \n",
    "                              severity_manager, log_classifier, seq_len, stride):\n",
    "    \"\"\"Process and format single log anomaly outputs - only for non-normal types\"\"\"\n",
    "    single_anomalies = single_errors > single_threshold\n",
    "    single_results = []\n",
    "    \n",
    "    for seq_idx, is_anomaly in enumerate(single_anomalies):\n",
    "        if is_anomaly:\n",
    "            # Get each log in the sequence\n",
    "            start_idx = seq_idx * stride\n",
    "            for log_offset in range(seq_len):\n",
    "                log_idx = start_idx + log_offset\n",
    "                if log_idx < len(original_df):\n",
    "                    log_entry = original_df.iloc[log_idx]\n",
    "                    \n",
    "                    # CHANGE: First pass - check log type classification\n",
    "                    classification = log_classifier.classify_log(\n",
    "                        log_entry.get('EventTemplate', ''),\n",
    "                        log_entry.get('Content', '')\n",
    "                    )\n",
    "                    \n",
    "                    # CHANGE: Only process if anomaly type is NOT normal\n",
    "                    if classification['log_type'] != 'normal':\n",
    "                        # Get severity\n",
    "                        error = single_errors[seq_idx]\n",
    "                        severity, confidence = severity_manager.classify_with_confidence(error)\n",
    "                        \n",
    "                        single_results.append({\n",
    "                            'log': {\n",
    "                                'content': log_entry.get('Content', ''),\n",
    "                                'event_template': log_entry.get('EventTemplate', ''),\n",
    "                                'level': log_entry.get('Level', ''),\n",
    "                                'component': log_entry.get('Component', ''),\n",
    "                                'line_id': log_entry.get('LineId', log_idx)\n",
    "                            },\n",
    "                            'anomaly_type': classification['log_type'],\n",
    "                            'severity': severity,\n",
    "                            'confidence': confidence,\n",
    "                            'timestamp': log_entry.get('Time', ''),\n",
    "                            'anomaly_score': float(error),\n",
    "                            'processing_mode': 'single_log'\n",
    "                        })\n",
    "    \n",
    "    return single_results\n",
    "\n",
    "def process_sequential_outputs(seq_errors, seq_threshold, test_data, original_df, \n",
    "                              severity_manager, log_classifier, seq_len, stride):\n",
    "    \"\"\"Process and format sequential anomaly outputs - only for non-normal types\"\"\"\n",
    "    seq_anomalies = seq_errors > seq_threshold\n",
    "    sequential_results = []\n",
    "    \n",
    "    for seq_idx, is_anomaly in enumerate(seq_anomalies):\n",
    "        if is_anomaly:\n",
    "            # Get the entire sequence\n",
    "            start_idx = seq_idx * stride\n",
    "            sequence_logs = []\n",
    "            sequence_classifications = []\n",
    "            \n",
    "            for log_offset in range(seq_len):\n",
    "                log_idx = start_idx + log_offset\n",
    "                if log_idx < len(original_df):\n",
    "                    log_entry = original_df.iloc[log_idx]\n",
    "                    \n",
    "                    # CHANGE: First pass - check log type for each log in sequence\n",
    "                    classification = log_classifier.classify_log(\n",
    "                        log_entry.get('EventTemplate', ''),\n",
    "                        log_entry.get('Content', '')\n",
    "                    )\n",
    "                    sequence_classifications.append(classification['log_type'])\n",
    "                    \n",
    "                    sequence_logs.append({\n",
    "                        'content': log_entry.get('Content', ''),\n",
    "                        'event_template': log_entry.get('EventTemplate', ''),\n",
    "                        'level': log_entry.get('Level', ''),\n",
    "                        'component': log_entry.get('Component', ''),\n",
    "                        'line_id': log_entry.get('LineId', log_idx),\n",
    "                        'timestamp': log_entry.get('Time', '')\n",
    "                    })\n",
    "            \n",
    "            # CHANGE: Check if sequence has any non-normal anomaly types\n",
    "            non_normal_types = [t for t in sequence_classifications if t != 'normal']\n",
    "            \n",
    "            # CHANGE: Only process if sequence contains non-normal anomaly types\n",
    "            if non_normal_types:\n",
    "                # Get severity for the sequence\n",
    "                error = seq_errors[seq_idx]\n",
    "                severity, confidence = severity_manager.classify_with_confidence(error)\n",
    "                \n",
    "                # Get most common non-normal anomaly type\n",
    "                from collections import Counter\n",
    "                if non_normal_types:\n",
    "                    anomaly_type_counts = Counter(non_normal_types)\n",
    "                    dominant_anomaly_type = anomaly_type_counts.most_common(1)[0][0]\n",
    "                else:\n",
    "                    # Fallback to most common overall type\n",
    "                    anomaly_type_counts = Counter(sequence_classifications)\n",
    "                    dominant_anomaly_type = anomaly_type_counts.most_common(1)[0][0]\n",
    "                \n",
    "                sequential_results.append({\n",
    "                    'logs': sequence_logs,\n",
    "                    'anomaly_type': dominant_anomaly_type,\n",
    "                    'severity': severity,\n",
    "                    'confidence': confidence,\n",
    "                    'timestamp': sequence_logs[0]['timestamp'] if sequence_logs else '',\n",
    "                    'sequence_length': len(sequence_logs),\n",
    "                    'anomaly_score': float(error),\n",
    "                    'processing_mode': 'sequential',\n",
    "                    'non_normal_count': len(non_normal_types),\n",
    "                    'total_logs_in_sequence': len(sequence_classifications)\n",
    "                })\n",
    "    \n",
    "    return sequential_results\n",
    "\n",
    "\n",
    "def display_single_log_results(results, max_display=10):\n",
    "    \"\"\"Display single log anomaly results\"\"\"\n",
    "    print(f\"\\n🔍 SINGLE LOG ANOMALIES ({len(results)} found - non-normal types only):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results[:max_display]):\n",
    "        print(f\"\\nAnomaly #{i+1}:\")\n",
    "        print(f\"Log: {result['log']['content'][:100]}...\")\n",
    "        print(f\"Anomaly Type: {result['anomaly_type']}\")\n",
    "        print(f\"Severity: {result['severity']}\")\n",
    "        print(f\"Timestamp: {result['timestamp']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"Score: {result['anomaly_score']:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    if len(results) > max_display:\n",
    "        print(f\"... and {len(results) - max_display} more\")\n",
    "\n",
    "def display_sequential_results(results, max_display=5):\n",
    "    \"\"\"Display sequential anomaly results\"\"\"\n",
    "    print(f\"\\n📊 SEQUENTIAL ANOMALIES ({len(results)} found - sequences with non-normal types only):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results[:max_display]):\n",
    "        print(f\"\\nSequence #{i+1}:\")\n",
    "        print(f\"Logs in sequence: {result['sequence_length']}\")\n",
    "        print(f\"Non-normal logs: {result['non_normal_count']}/{result['total_logs_in_sequence']}\")\n",
    "        print(f\"Dominant Anomaly Type: {result['anomaly_type']}\")\n",
    "        print(f\"Severity: {result['severity']}\")\n",
    "        print(f\"Timestamp: {result['timestamp']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"Score: {result['anomaly_score']:.4f}\")\n",
    "        \n",
    "        print(\"Sequence logs:\")\n",
    "        for j, log in enumerate(result['logs'][:3]):  # Show first 3 logs\n",
    "            print(f\"  {j+1}. {log['content'][:80]}...\")\n",
    "        if len(result['logs']) > 3:\n",
    "            print(f\"  ... and {len(result['logs']) - 3} more logs\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    if len(results) > max_display:\n",
    "        print(f\"... and {len(results) - max_display} more sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e641a6d-1f73-42a4-bf9e-b2792ff3eeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Removed 'Date' column - temporal identifiers cause overfitting\n",
      "Removed 'PID' column - process IDs are not semantically meaningful\n",
      "Dropping columns: ['LineId', 'Time', 'Date', 'PID']\n",
      "EventTemplate has 519 unique values\n",
      "Including EventTemplate as categorical feature\n",
      "Processing Content column...\n",
      "Extracted features: length, word_count, has_error, has_warning, has_large_numbers, has_critical, has_network, has_memory\n",
      "  content_has_error: 0.2% of logs\n",
      "  content_has_warning: 0.9% of logs\n",
      "  content_has_large_numbers: 57.3% of logs\n",
      "  content_has_critical: 40.9% of logs\n",
      "  content_has_network: 14.1% of logs\n",
      "  content_has_memory: 41.0% of logs\n",
      "Removed 'Month' column - avoiding temporal overfitting\n",
      "Using categorical columns: ['Level', 'Component', 'EventId', 'EventTemplate']\n",
      "Processing EventTemplate with TF-IDF for better semantic representation...\n",
      "EventTemplate TF-IDF generated 50 meaningful features\n",
      "Other categoricals generated 101 features\n",
      "Numerical columns: ['content_length', 'content_word_count', 'content_has_error', 'content_has_warning', 'content_has_large_numbers', 'content_has_critical', 'content_has_network', 'content_has_memory']\n",
      "Final feature dimension: 159\n",
      "Numerical features: 8, Categorical features: 151\n",
      "Data shape: (25456, 159)\n",
      "Training Hybrid Ensemble Models...\n",
      "\n",
      "=== Training Hybrid Model 1/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.8688 | Val Loss: 0.5688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.6017 | Val Loss: 0.5132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.5644 | Val Loss: 0.4886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss: 0.5541 | Val Loss: 0.4683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss: 0.5441 | Val Loss: 0.4639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 44 epochs\n",
      "\n",
      "=== Training Hybrid Model 2/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.8566 | Val Loss: 0.5736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.5791 | Val Loss: 0.5024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.5362 | Val Loss: 0.4834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss: 0.5089 | Val Loss: 0.4419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss: 0.4931 | Val Loss: 0.4317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Train Loss: 0.4841 | Val Loss: 0.4261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: Train Loss: 0.4770 | Val Loss: 0.4139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: Train Loss: 0.4717 | Val Loss: 0.4098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 75 epochs\n",
      "\n",
      "=== Training Hybrid Model 3/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.7993 | Val Loss: 0.5450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.5029 | Val Loss: 0.4360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.4411 | Val Loss: 0.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss: 0.4096 | Val Loss: 0.3722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss: 0.3850 | Val Loss: 0.3576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Train Loss: 0.3644 | Val Loss: 0.3426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: Train Loss: 0.3501 | Val Loss: 0.3372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: Train Loss: 0.3330 | Val Loss: 0.3258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: Train Loss: 0.3200 | Val Loss: 0.3163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train Loss: 0.3169 | Val Loss: 0.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Ensemble weights: ['0.276', '0.314', '0.410']\n",
      "\n",
      "Evaluating different processing modes...\n",
      "Processing sequences...\n",
      "Processing individual logs...\n",
      "Sequential anomalies detected: 24\n",
      "Single log anomalies detected: 24\n",
      "\n",
      "Initializing severity classification...\n",
      "✅ Learned severity thresholds: {'p85': 2.730556753315499, 'p95': 4.080528542445025, 'p99': 7.0022962475051465}\n",
      "📊 Error distribution stats: {'mean': 1.4596107065258013, 'std': 1.512213478614884, 'median': 0.9437472978112085, 'iqr': 0.9271088474004039}\n",
      "\n",
      "Initializing rule-based log classification...\n",
      "\n",
      "Processing single log outputs...\n",
      "\n",
      "Processing sequential outputs...\n",
      "Single log anomalies found: 56\n",
      "Sequential anomalies found: 13\n",
      "\n",
      "🔍 SINGLE LOG ANOMALIES (56 found - non-normal types only):\n",
      "================================================================================\n",
      "\n",
      "Anomaly #1:\n",
      "Log: ALERT exited abnormally with [1]...\n",
      "Anomaly Type: system_critical\n",
      "Severity: Critical\n",
      "Timestamp: 04:02:55\n",
      "Confidence: 0.722\n",
      "Score: 8.5563\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #2:\n",
      "Log: Kerberos authentication failed...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: Critical\n",
      "Timestamp: 20:53:06\n",
      "Confidence: 1.000\n",
      "Score: 14.2029\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #3:\n",
      "Log: Kerberos authentication failed...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: Critical\n",
      "Timestamp: 20:53:06\n",
      "Confidence: 1.000\n",
      "Score: 14.2029\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #4:\n",
      "Log: Kerberos authentication failed...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: Critical\n",
      "Timestamp: 20:53:06\n",
      "Confidence: 1.000\n",
      "Score: 14.2029\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #5:\n",
      "Log: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: High\n",
      "Timestamp: 10:56:41\n",
      "Confidence: 1.000\n",
      "Score: 6.7830\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #6:\n",
      "Log: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: High\n",
      "Timestamp: 10:56:41\n",
      "Confidence: 1.000\n",
      "Score: 6.7830\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #7:\n",
      "Log: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: High\n",
      "Timestamp: 10:56:41\n",
      "Confidence: 1.000\n",
      "Score: 6.7830\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #8:\n",
      "Log: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: High\n",
      "Timestamp: 10:56:42\n",
      "Confidence: 1.000\n",
      "Score: 6.7830\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #9:\n",
      "Log: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: High\n",
      "Timestamp: 10:56:42\n",
      "Confidence: 1.000\n",
      "Score: 6.7830\n",
      "----------------------------------------\n",
      "\n",
      "Anomaly #10:\n",
      "Log: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root...\n",
      "Anomaly Type: authentication_error\n",
      "Severity: High\n",
      "Timestamp: 10:56:42\n",
      "Confidence: 0.967\n",
      "Score: 5.9866\n",
      "----------------------------------------\n",
      "... and 46 more\n",
      "\n",
      "📊 SEQUENTIAL ANOMALIES (13 found - sequences with non-normal types only):\n",
      "================================================================================\n",
      "\n",
      "Sequence #1:\n",
      "Logs in sequence: 8\n",
      "Non-normal logs: 1/8\n",
      "Dominant Anomaly Type: system_critical\n",
      "Severity: Critical\n",
      "Timestamp: 03:40:59\n",
      "Confidence: 0.676\n",
      "Score: 8.2341\n",
      "Sequence logs:\n",
      "  1. connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005...\n",
      "  2. session opened for user cyrus by (uid=0)...\n",
      "  3. session closed for user cyrus...\n",
      "  ... and 5 more logs\n",
      "----------------------------------------\n",
      "\n",
      "Sequence #2:\n",
      "Logs in sequence: 8\n",
      "Non-normal logs: 3/8\n",
      "Dominant Anomaly Type: authentication_error\n",
      "Severity: Critical\n",
      "Timestamp: 20:53:06\n",
      "Confidence: 1.000\n",
      "Score: 13.6751\n",
      "Sequence logs:\n",
      "  1. Kerberos authentication failed...\n",
      "  2. Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connec...\n",
      "  3. Kerberos authentication failed...\n",
      "  ... and 5 more logs\n",
      "----------------------------------------\n",
      "\n",
      "Sequence #3:\n",
      "Logs in sequence: 8\n",
      "Non-normal logs: 5/8\n",
      "Dominant Anomaly Type: authentication_error\n",
      "Severity: High\n",
      "Timestamp: 09:14:44\n",
      "Confidence: 1.000\n",
      "Score: 6.9933\n",
      "Sequence logs:\n",
      "  1. session opened for user test by (uid=509)...\n",
      "  2. session closed for user test...\n",
      "  3. session closed for user test...\n",
      "  ... and 5 more logs\n",
      "----------------------------------------\n",
      "\n",
      "Sequence #4:\n",
      "Logs in sequence: 8\n",
      "Non-normal logs: 5/8\n",
      "Dominant Anomaly Type: authentication_error\n",
      "Severity: High\n",
      "Timestamp: 10:56:42\n",
      "Confidence: 0.972\n",
      "Score: 6.0083\n",
      "Sequence logs:\n",
      "  1. authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129....\n",
      "  2. authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129....\n",
      "  3. authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129....\n",
      "  ... and 5 more logs\n",
      "----------------------------------------\n",
      "\n",
      "Sequence #5:\n",
      "Logs in sequence: 8\n",
      "Non-normal logs: 1/8\n",
      "Dominant Anomaly Type: system_critical\n",
      "Severity: Critical\n",
      "Timestamp: 01:41:33\n",
      "Confidence: 0.501\n",
      "Score: 7.0124\n",
      "Sequence logs:\n",
      "  1. session opened for user test by (uid=509)...\n",
      "  2. session opened for user test by (uid=509)...\n",
      "  3. session closed for user test...\n",
      "  ... and 5 more logs\n",
      "----------------------------------------\n",
      "... and 8 more sequences\n",
      "\n",
      "📈 COMPARISON SUMMARY:\n",
      "Single Log Processing:\n",
      "   • Total anomalies: 56\n",
      "   • Mean error: 1.4694\n",
      "   • Threshold: 4.1593\n",
      "\n",
      "Sequential Processing:\n",
      "   • Total anomalies: 13\n",
      "   • Mean error: 1.4498\n",
      "   • Threshold: 4.0615\n",
      "\n",
      "Severity Distribution:\n",
      "Single Logs: Counter({'High': 43, 'Critical': 13})\n",
      "Sequential: Counter({'High': 9, 'Critical': 4})\n",
      "\n",
      "✅ Results saved to:\n",
      "   • single_log_anomalies.json\n",
      "   • sequential_anomalies.json\n",
      "   • anomaly_outputs.json\n",
      "   • hybrid_ensemble_artifacts.pkl\n",
      "\n",
      "✅ Hybrid testing complete! Ready for mode-specific analysis.\n"
     ]
    }
   ],
   "source": [
    "# ================== MAIN EXECUTION ==================\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    DATA_PATH = \"data/logs/processed/Linux.log_structured.csv\"\n",
    "    SEQ_LEN = 8\n",
    "    STRIDE = 8\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    # Load and preprocess data\n",
    "    data, ohe, imputer, scaler, original_df = load_and_preprocess(DATA_PATH)\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_size = int(0.7 * len(data))\n",
    "    val_size = int(0.15 * len(data))\n",
    "    \n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "\n",
    "    train_dataset = LogDataset(train_data, SEQ_LEN, STRIDE)\n",
    "    val_dataset = LogDataset(val_data, SEQ_LEN, STRIDE)\n",
    "    test_dataset = LogDataset(test_data, SEQ_LEN, STRIDE)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize hybrid ensemble\n",
    "    input_dim = data.shape[1]\n",
    "    ensemble = HybridEnsembleDetector(input_dim, num_models=3, enable_single_log=True)\n",
    "\n",
    "    # Train ensemble\n",
    "    print(\"Training Hybrid Ensemble Models...\")\n",
    "    model_performances = ensemble.train_ensemble(train_loader, val_loader, patience=5)\n",
    "\n",
    "    # Evaluate with different processing modes\n",
    "    print(\"\\nEvaluating different processing modes...\")\n",
    "\n",
    "    # Sequential processing\n",
    "    print(\"Processing sequences...\")\n",
    "    seq_errors, _ = ensemble.predict(test_loader, mode='sequential')\n",
    "    seq_threshold = np.percentile(seq_errors, 95)\n",
    "    seq_anomalies = seq_errors > seq_threshold\n",
    "\n",
    "    # Single log processing  \n",
    "    print(\"Processing individual logs...\")\n",
    "    single_errors, _ = ensemble.predict(test_loader, mode='single')\n",
    "    single_threshold = np.percentile(single_errors, 95)\n",
    "    single_anomalies = single_errors > single_threshold\n",
    "\n",
    "    print(f\"Sequential anomalies detected: {seq_anomalies.sum()}\")\n",
    "    print(f\"Single log anomalies detected: {single_anomalies.sum()}\")\n",
    "\n",
    "    # Initialize severity manager and classifier\n",
    "    print(\"\\nInitializing severity classification...\")\n",
    "    severity_manager = EnhancedSeverityManager(\n",
    "        percentiles=[85, 95, 99],\n",
    "        severity_labels=['Low', 'Medium', 'High', 'Critical']\n",
    "    )\n",
    "\n",
    "    # Learn thresholds from both processing modes\n",
    "    all_errors = np.concatenate([seq_errors, single_errors])\n",
    "    severity_manager.learn_thresholds(all_errors)\n",
    "\n",
    "    print(\"\\nInitializing rule-based log classification...\")\n",
    "    log_classifier = RuleBasedLogClassifier()\n",
    "\n",
    "    # Process outputs separately\n",
    "    print(\"\\nProcessing single log outputs...\")\n",
    "    single_log_results = process_single_log_outputs(\n",
    "        single_errors, single_threshold, test_data, original_df,\n",
    "        severity_manager, log_classifier, SEQ_LEN, STRIDE\n",
    "    )\n",
    "\n",
    "    print(\"\\nProcessing sequential outputs...\")\n",
    "    sequential_results = process_sequential_outputs(\n",
    "        seq_errors, seq_threshold, test_data, original_df,\n",
    "        severity_manager, log_classifier, SEQ_LEN, STRIDE\n",
    "    )\n",
    "\n",
    "    print(f\"Single log anomalies found: {len(single_log_results)}\")\n",
    "    print(f\"Sequential anomalies found: {len(sequential_results)}\")\n",
    "\n",
    "    # Display results\n",
    "    display_single_log_results(single_log_results)\n",
    "    display_sequential_results(sequential_results)\n",
    "\n",
    "    # Save structured outputs\n",
    "    output_data = {\n",
    "        'single_log_anomalies': single_log_results,\n",
    "        'sequential_anomalies': sequential_results,\n",
    "        'summary': {\n",
    "            'total_single_anomalies': len(single_log_results),\n",
    "            'total_sequential_anomalies': len(sequential_results),\n",
    "            'processing_timestamp': datetime.now().isoformat(),\n",
    "            'thresholds': {\n",
    "                'single_log_threshold': float(single_threshold),\n",
    "                'sequential_threshold': float(seq_threshold)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save to JSON files\n",
    "    with open('single_log_anomalies.json', 'w') as f:\n",
    "        json.dump(single_log_results, f, indent=2, default=str)\n",
    "\n",
    "    with open('sequential_anomalies.json', 'w') as f:\n",
    "        json.dump(sequential_results, f, indent=2, default=str)\n",
    "\n",
    "    with open('anomaly_outputs.json', 'w') as f:\n",
    "        json.dump(output_data, f, indent=2, default=str)\n",
    "\n",
    "    # Save model artifacts\n",
    "    artifacts = {\n",
    "        'ohe': ohe,\n",
    "        'imputer': imputer,\n",
    "        'scaler': scaler,\n",
    "        'single_log_threshold': single_threshold,\n",
    "        'sequential_threshold': seq_threshold,\n",
    "        'ensemble_weights': ensemble.weights,\n",
    "        'input_dim': input_dim,\n",
    "        'seq_len': SEQ_LEN,\n",
    "        'stride': STRIDE,\n",
    "        'severity_manager': severity_manager,\n",
    "        'processing_modes_available': ['sequential', 'single', 'hybrid']\n",
    "    }\n",
    "\n",
    "    with open('hybrid_ensemble_artifacts.pkl', 'wb') as f:\n",
    "        pickle.dump(artifacts, f)\n",
    "\n",
    "    # Comparison statistics\n",
    "    print(f\"\\n📈 COMPARISON SUMMARY:\")\n",
    "    print(f\"Single Log Processing:\")\n",
    "    print(f\"   • Total anomalies: {len(single_log_results)}\")\n",
    "    print(f\"   • Mean error: {np.mean(single_errors):.4f}\")\n",
    "    print(f\"   • Threshold: {single_threshold:.4f}\")\n",
    "\n",
    "    print(f\"\\nSequential Processing:\")\n",
    "    print(f\"   • Total anomalies: {len(sequential_results)}\")\n",
    "    print(f\"   • Mean error: {np.mean(seq_errors):.4f}\")\n",
    "    print(f\"   • Threshold: {seq_threshold:.4f}\")\n",
    "\n",
    "    # Severity distribution for both modes\n",
    "    single_severities = [r['severity'] for r in single_log_results]\n",
    "    seq_severities = [r['severity'] for r in sequential_results]\n",
    "\n",
    "    print(f\"\\nSeverity Distribution:\")\n",
    "    print(f\"Single Logs: {Counter(single_severities)}\")\n",
    "    print(f\"Sequential: {Counter(seq_severities)}\")\n",
    "\n",
    "    print(f\"\\n✅ Results saved to:\")\n",
    "    print(f\"   • single_log_anomalies.json\")\n",
    "    print(f\"   • sequential_anomalies.json\") \n",
    "    print(f\"   • anomaly_outputs.json\")\n",
    "    print(f\"   • hybrid_ensemble_artifacts.pkl\")\n",
    "\n",
    "    print(f\"\\n✅ Hybrid testing complete! Ready for mode-specific analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7ee45-7c61-4128-a8dc-30339449c9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-SIEM",
   "language": "python",
   "name": "ai-siem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
